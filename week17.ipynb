{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7wpF9RijJo-"
   },
   "source": [
    "<h2 style='center'>Web Crawler: getting webpages</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTqvGF981BPE"
   },
   "source": [
    "### requests 模組: get webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "id": "Pd776i6Zv_m1",
    "outputId": "216de5d5-e11e-49c1-9f3e-62b61188b225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n",
      "<html><head><!-- saved from url=(0064)http://www.syps.tp.edu.tw/english1200/syps_eng/level1-8.html -->\n",
      "\n",
      "<meta content=\"IE=7.0000\" http-equiv=\"X-UA-Compatible\"><title>Âù¶é°ê¤p­^¤å³æ¦r1200¦Cªí</title>\n",
      "\n",
      "<meta content=\"text/html; charset=big5\" http-equiv=\"Content-Type\">\n",
      "<meta name=\"GENERATOR\" content=\"MSHTML 10.00.9200.16540\"></head><body>\n",
      "<div class=\"WordSection1\">\n",
      "<p class=\"MsoNormal\"><span lang=\"EN-US\"></span><span style=\"font-family: '·s²Ó©úÅé','serif';\"><br></span></p><p style=\"font-weight: bold; color: red;\" class=\"MsoNormal\"><span style=\"font-family: '·s²Ó©úÅé','serif';\">²Ä</span><span lang=\"EN-US\">8</span><span style=\"font-family: '·s²Ó©úÅé','serif';\">¯Å</span><span lang=\"EN-US\">p1 able ~ yummy</span></p>\n",
      "<table class=\"MsoTableGrid\" style=\"border: medium none ; border-collapse: collapse;\" border=\"1\" cellpadding=\"0\" cellspacing=\"0\">\n",
      "  <tbody>\n",
      "  <tr>\n",
      "    <td style=\"border: 1pt solid windowtext; padding: 0cm 5.4\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://stuweb.syps.tp.edu.tw/english1200/syps_eng/level8.html'\n",
    "r = requests.get(url)\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "id": "x4W4sxRaxUFi",
    "outputId": "795cb59c-d6b9-417f-8d85-40ab3231ecda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n",
      "<html><head><!-- saved from url=(0064)http://www.syps.tp.edu.tw/english1200/syps_eng/level1-8.html -->\n",
      "\n",
      "<meta content=\"IE=7.0000\" http-equiv=\"X-UA-Compatible\"><title>雙園國小英文單字1200列表</title>\n",
      "\n",
      "<meta content=\"text/html; charset=big5\" http-equiv=\"Content-Type\">\n",
      "<meta name=\"GENERATOR\" content=\"MSHTML 10.00.9200.16540\"></head><body>\n",
      "<div class=\"WordSection1\">\n",
      "<p class=\"MsoNormal\"><span lang=\"EN-US\"></span><span style=\"font-family: '新細明體','serif';\"><br></span></p><p style=\"font-weight: bold; color: red;\" class=\"MsoNormal\"><span style=\"font-family: '新細明體','serif';\">第</span><span lang=\"EN-US\">8</span><span style=\"font-family: '新細明體','serif';\">級</span><span lang=\"EN-US\">p1 able ~ yummy</span></p>\n",
      "<table class=\"MsoTableGrid\" style=\"border: medium none ; border-collapse: collapse;\" border=\"1\" cellpadding=\"0\" cellspacing=\"0\">\n",
      "  <tbody>\n",
      "  <tr>\n",
      "    <td style=\"border: 1pt solid windowtext; padding: 0cm 5.4pt; width: 139.35pt; fon\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url)\n",
    "r.encoding='big5'\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "colab_type": "code",
    "id": "-SX4Nhwdks-R",
    "outputId": "8d79433e-2346-4d68-ccc5-4421a2390da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"zh-tw\">\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
      "<meta name=\"keywords\" content=\"國立屏東大學,NPTU,屏東大學,屏大,屏商,屏商院,屏東商業技術學院,國立屏東商業技術學院,屏教大,屏東教育大學,國立屏東教育大學\" />\n",
      "<meta name=\"description\" content=\"國立屏東大學,NPTU,簡稱屏東大學,2014年8月1日由國立屏東教育大學與國立屏東商業技術學院合併成立。校址：屏東市民生路4-18號。屏商校區：屏東市民生東路51號\" />\n",
      "<!-- Global site tag (gtag.js) - Google Analytics -->\n",
      "<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-132745216-1\"></script>\n",
      "<script>\n",
      "  window.dataLayer = window.dataLayer || [];\n",
      "  function gtag(){dataLayer.push(arguments);}\n",
      "  gtag('js', new Date());\n",
      "\n",
      "  gtag('config', 'UA-132745216-1');\n",
      "</script>\n",
      "<title>國立屏東大學 NPTU </title>\n",
      "\n",
      "<link rel=\"stylesheet\" href=\"/ezfiles/0/1000/static/combine-zh-tw.css\" type=\"text/css\" />\n",
      "\n",
      "<!--[if lte IE 6]> \n",
      "<link rel=\"stylesheet\" href=\"/style/style-ie6.css\" type=\"text/cs\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('https://www.nptu.edu.tw/bin/home.php')\n",
    "r.encoding='utf-8'\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "dfJU9ubexBRv",
    "outputId": "c68484c8-02f0-45ca-af08-08b5cad9c844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<html>\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Language\" content=\"zh-tw\">\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=big5\">\n",
      "<meta name=\"GENERATOR\" content=\"Microsoft FrontPage 4.0\">\n",
      "<meta name=\"ProgId\" content=\"FrontPage.Editor.Document\">\n",
      "<title>每日一句</title>\n",
      "<script language=\"JavaScript\" src=\"../../_include/topjs.js\"></script>\n",
      "<link href=\"../../CSS/index.css\" rel=\"stylesheet\" type=\"text/css\">\n",
      "<script language=\"JavaScript\" src=\"../../mm_menu.js\"></script>\n",
      "<script type=\"text/javascript\" src=\"http://www.liveabc.com/site/scripts/js_main.js\"></script>\n",
      "<script type=\"text/javascript\" src=\"../js/jquery.min.js\"></script>\n",
      "<script type=\"text/javascript\">\n",
      "$(function(){\n",
      "\t$('.addressClick').click(function(){\n",
      "\t\tvar addressValue = $(this).attr(\"id\");\n",
      "\t\t$('embed').remove();\n",
      "\t\t$('body').append('<embed src=\"' + addressValue + '\" autostart=\"true\" hidden=\"true\" loop=\"false\">');\n",
      "\t});\n",
      "});\n",
      "</script>\n",
      "<style>\n",
      "P {font-size:10pt; line-height:13pt; color:#333300}\n",
      "td {font-size:10\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.liveabc.com/site/daily_sentence/dailysentence_main.asp'\n",
    "r = requests.get(url)\n",
    "r.encoding='big5'\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BixltCjgq37s"
   },
   "source": [
    "#### 許多網站禁止爬蟲程式，解法：User-Agent for web-browser\n",
    "ref: [freelancer僞裝user-agent]('https://freelancerlife.info/zh/blog/python-web-scraping-user-agent-for-shopee/'), [check your own useragent](https://www.mkyong.com/computer-tips/how-to-view-http-headers-in-google-chrome/)\n",
    "- On chrome: first connect to a webpage, then press F12 to enter into developer mode \n",
    "- Click on the Network tab\n",
    "- Press Ctrl+R, then click the first link, you can see the user-agent field on the right (scroll-down to the last part)\n",
    "\n",
    "use web browser's user agent as the identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "8d9374Mmoj-3",
    "outputId": "4e322fad-bb4c-4f2d-9f16-156581f1163b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fake-useragent\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
      "Building wheels for collected packages: fake-useragent\n",
      "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=6187d19cbc5098aafdb8e19252cd15f9340a0be2e68a888b0a3e2abf71e01c94\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
      "Successfully built fake-useragent\n",
      "Installing collected packages: fake-useragent\n",
      "Successfully installed fake-useragent-0.1.11\n"
     ]
    }
   ],
   "source": [
    "!pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yIvX7SjZq3DJ",
    "outputId": "69717ba5-b924-4d3f-fc68-0b29e9b08203"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "UserAgent().random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "ChfKdyeAx98q",
    "outputId": "0ef445f7-dab9-4582-f081-9bab2356a368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"zh-tw\">\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=EmulateIE7\" /><meta name=\"keywords\" content=\"國立屏東大學,屏東大學,教務處,課務組,屏東,nptu,選課,加退選,校際選課,停修,超修,教學評量,全英語,遴聘業師,課程大綱,課程地圖,行事曆\" />\n",
      "<meta name=\"description\" content=\"國立屏東大學 教務處 課務組網站提供學生本學期各類選課資訊及課程人工加退選、校際選課、超修、停修申請等業務窗口。\" />\n",
      "<meta name=\"google-site-verification\" content=\"pwWQ65U_z58hDbQgP2sCwXx36Oo49b9u7W_RdJUTb0I\" />\n",
      "<title>108學年度第2學期選課時程表【在校生(含延畢)/復學生/寒轉生/提前入學研究生/交換生】 - 課務組-國立屏東大學教務處 </title>\n",
      "\n",
      "<link rel=\"stylesheet\" href=\"/ezfiles/50/1050/static/combine-zh-tw.css\" type=\"text/css\" />\n",
      "\n",
      "<!--[if lte IE 6]> \n",
      "<link rel=\"stylesheet\" href=\"/style/style-ie6.css\" type=\"text/css\" /> \n",
      "<![endif]-->\n",
      "<Link rel=\"SHORTCUT ICON\"  href=\"/ezfiles/50/1050/sys_1050_8954908_02932.ico\" type=\"image/x-icon\" />\n",
      "<link \n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.cud.nptu.edu.tw/files/13-1050-91619.php?Lang=zh-tw'\n",
    "r = requests.session()  # a session is the duration of connection\n",
    "r.headers['user-agent']=UserAgent().random\n",
    "res=r.get(url)\n",
    "res.encoding='utf-8'\n",
    "print(res.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d2UQuGaQ5rKH"
   },
   "source": [
    "#### Interaction with forms \n",
    "Via *get* or *post* method\n",
    "- get is simpler, `get`(url, `params`=data)\n",
    "- post is complex but secure, `post`(url, `data`=data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "FYwPD5wOr4V5",
    "outputId": "8b6f3570-5742-4412-a5e3-140ed3a21e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/SearchResultsPage\" lang=\"en\"><head><meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"><meta content=\"origin\" name=\"referrer\"><title>fake news detection - Google Search</title><script nonce=\"T2kPctb68hWgxhaw+ejdzQ==\">(function(){window.google={kEI:'3awKXtm3C8aP9QPZt4ngBw',kEXPI:'31',authuser:0,kscs:'c9c918f0_3awKXtm3C8aP9QPZt4ngBw',kGL:'US',kBL:'v77x'};google.sn='web';google.kHL='en';google.jsfs='Ffpdje';})();(function(){google.lc=[];google.li=0;google.getEI=function(a){for(var b;a&&(!a.getAttribute||!(b=a.getAttribute(\"eid\")));)a=a.parentNode;return b||google.kEI};google.getLEI=function(a){for(var b=null;a&&(!a.getAttribute||!(b=a.getAttribute(\"leid\")));)a=a.parentNode;return b};google.https=function(){return\"https:\"==window.location.protocol};google.ml=function(){return null};google.time=function(){return(new Date).getTime()};google.log=function(a,b,e,c,g){if(a=google.logUrl(a,b,e,\n"
     ]
    }
   ],
   "source": [
    "q = input('search terms: ')\n",
    "url='https://www.google.com/search'\n",
    "with requests.session() as s:\n",
    "    s.headers['user-agent'] = UserAgent().random        \n",
    "    r    = s.get(url, params={'q':q})\n",
    "    print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "_EtfC2_vvtmk",
    "outputId": "2e75a5a3-53c4-4f82-b388-35b497ed4ff1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search terms: hello world\n",
      "<!doctype html><html itemscope=\"\" itemtype=\"http://schema.org/SearchResultsPage\" lang=\"en\"><head><meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"><meta content=\"origin\" name=\"referrer\"><title>hello world - Google Search</title><script nonce=\"VRoypjkbVTFtFfwH3uPMxQ==\">(function(){window.google={kEI:'iLAKXo_cFdLarQHahYDABA',kEXPI:'31',authuser:0,kscs:'c9c918f0_iLAKXo_cFdLarQHahYDABA',kGL:'US',kBL:'v77x'};google.sn='web';google.kHL='en';google.jsfs='Ffpdje';})();(function(){google.lc=[];google.li=0;google.getEI=function(a){for(var b;a&&(!a.getAttribute||!(b=a.getAttribute(\"eid\")));)a=a.parentNode;return b||google.kEI};google.getLEI=function(a){for(var b=null;a&&(!a.getAttribute||!(b=a.getAttribute(\"leid\")));)a=a.parentNode;return b};google.https=function(){return\"https:\"==window.location.protocol};google.ml=function(){return null};google.time=function(){return(new Date).getTime()};google.log=function(a,b,e,c,g){if(a=google.logUrl(a,b,e,c,g)){b=\n"
     ]
    }
   ],
   "source": [
    "q = input('search terms: ')\n",
    "url=f'https://www.google.com/search?q={q.replace(\" \",\"+\")}'\n",
    "with requests.session() as s:\n",
    "    s.headers['user-agent'] = UserAgent().random        \n",
    "    r    = s.get(url)\n",
    "    print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "4wJn0e7j9YrL",
    "outputId": "b80cb89c-cfef-4bbf-f177-93383ebd99d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"bbs-screen bbs-content center clear\">\n",
      "    <form action=\"/ask/over18\" method=\"post\">\n",
      "        <input type=\"hidden\" name=\"from\" value=\"/bbs/Gossiping/index.html\">\n",
      "        <div class=\"over18-button-container\">\n",
      "            <button class=\"btn-big\" type=\"submit\" name=\"yes\" value=\"yes\">我同意，我已年滿十八歲<br><small>進入</small></button>\n",
      "        </div>\n",
      "        <div class=\"over18-button-container\">\n",
      "            <button class=\"btn-big\" type=\"submit\" name=\"no\" value=\"no\">未滿十八歲或不同意本條款<br><small>離開</small></button>\n",
      "        </div>\n",
      "    </form>\n",
      "</div>\n",
      "\n",
      "\t\t\n",
      "\n",
      "<script>\n",
      "  (function(i,s,o,g,r,a,\n"
     ]
    }
   ],
   "source": [
    "url='https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "with requests.session() as s:\n",
    "    s.headers['user-agent'] = UserAgent().random \n",
    "    r = s.get(url)\n",
    "    print(r.text[900:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "xmJYv3fN61Pt",
    "outputId": "0c9ce3cf-8db5-4d27-8b42-1019afc6f8b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ptt.cc/bbs/Gossiping/index.html\n",
      "ossiping</a>\n",
      "\t\t<a class=\"right small\" href=\"/about.html\">關於我們</a>\n",
      "\t\t<a class=\"right small\" href=\"/contact.html\">聯絡資訊</a>\n",
      "\t</div>\n",
      "</div>\n",
      "\n",
      "<div id=\"main-container\">\n",
      "\t<div id=\"action-bar-container\">\n",
      "\t\t<div class=\"action-bar\">\n",
      "\t\t\t<div class=\"btn-group btn-group-dir\">\n",
      "\t\t\t\t<a class=\"btn selected\" href=\"/bbs/Gossiping/index.html\">看板</a>\n",
      "\t\t\t\t<a class=\"btn\" href=\"/man/Gossiping/index.html\">精華區</a>\n",
      "\t\t\t</div>\n",
      "\t\t\t<div class=\"btn-group btn-group-paging\">\n",
      "\t\t\t\t<a class=\"btn wide\" href=\"/bbs/Gossiping/index1.html\">最舊</a>\n",
      "\t\t\t\t<a class=\"btn wide\" href=\"/bbs/Gossiping/index38825.html\">&lsaquo; 上頁</a>\n",
      "\t\t\t\t<a clas\n"
     ]
    }
   ],
   "source": [
    "url='https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "url1='https://www.ptt.cc/ask/over18'\n",
    "with requests.session() as s:\n",
    "    s.headers['user-agent'] = UserAgent().random        \n",
    "    s.post(url1, data={'yes':'x'})\n",
    "    r = s.get(url)\n",
    "    print(r.url)\n",
    "    print(r.text[900:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWE2t39qoROL"
   },
   "source": [
    "### Beautiful soup select\n",
    "- for more: https://gist.github.com/yoki/b7f2fcef64c893e307c4c59303ead19a\n",
    "- soup.select('a[href=\"http://example.com/elsie\"]') # exact attribute\n",
    "- soup.select('a[href^=\"http://example.com/\"]') # start match\n",
    "- soup.select('a[href$=\"tillie\"]') # end match\n",
    "- soup.select('a[href*=\".com/el\"]') # middle match\n",
    "- soup.select('div[class^=\"def ddef_d\"]') # start match\n",
    "- soup.find_all('span', class_='def ddef_d')\n",
    "- soup.find_all(['span', 'div']) # using a list for multiple tags\n",
    "- soup.find_all('span', class_=re.compile(r'^def'))\n",
    "- soup.find_all('span', {'class':re.compiler(r'^def')})\n",
    "- soup.find('span', class_='def ddef_d')\n",
    "- soup.find('span', {'class': 'def ddef_d'})\n",
    "- soup.select_one('div[class^=\"def ddef_d\"]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wi_syh_umHHW"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWpk8iZrptyO"
   },
   "outputs": [],
   "source": [
    "data = '''<div class='def ddef_d'>\n",
    "<span class='listen'>\n",
    "<b>This is a test.</b>\n",
    "</span>\n",
    "<span class='listen2'>\n",
    "This is a test 2.\n",
    "</span>\n",
    "<span class='listen3'>\n",
    "this is a test 3.\n",
    "</span>\n",
    "<span class='no-listen1'>\n",
    "this isn't a test 1.\n",
    "</span>\n",
    "<span class='no-listen2'>\n",
    "this isn't a test 2.\n",
    "</span>\n",
    "</div>\n",
    "<div class='def ddef_d2'>\n",
    "<span>\n",
    "  I Like\n",
    "  <span class='unwanted'> to punch </span>\n",
    "  <span class='unwanted'> 2nd unwanted </span>\n",
    "   your face\n",
    " <span>\n",
    " </div>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oi8OE28q2rW"
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "colab_type": "code",
    "id": "w242ZLmIrKtM",
    "outputId": "1267ef40-1fa2-4441-c57a-884c60505467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      " </head>\n",
      " <body>\n",
      "  <div class=\"def ddef_d\">\n",
      "   <span class=\"listen\">\n",
      "    <b>\n",
      "     This is a test.\n",
      "    </b>\n",
      "   </span>\n",
      "   <span class=\"listen2\">\n",
      "    This is a test 2.\n",
      "   </span>\n",
      "   <span class=\"listen3\">\n",
      "    this is a test 3.\n",
      "   </span>\n",
      "   <span class=\"no-listen1\">\n",
      "    this isn't a test 1.\n",
      "   </span>\n",
      "   <span class=\"no-listen2\">\n",
      "    this isn't a test 2.\n",
      "   </span>\n",
      "  </div>\n",
      "  <div class=\"def ddef_d2\">\n",
      "   <span>\n",
      "    I Like\n",
      "    <span class=\"unwanted\">\n",
      "     to punch\n",
      "    </span>\n",
      "    <span class=\"unwanted\">\n",
      "     2nd unwanted\n",
      "    </span>\n",
      "    your face\n",
      "    <span>\n",
      "    </span>\n",
      "   </span>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "hrDnRoErrAjx",
    "outputId": "76daa0eb-27a5-422a-e6b8-e542bc66fd7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test.\n",
      "This is a test 2.\n",
      "this is a test 3.\n",
      "this isn't a test 1.\n",
      "this isn't a test 2.\n",
      "I Like\n",
      "   to punch \n",
      "   2nd unwanted \n",
      "   your face\n",
      "to punch\n",
      "2nd unwanted\n",
      "--------------------------------------------------------------------------------\n",
      "[<b>This is a test.</b>]\n"
     ]
    }
   ],
   "source": [
    "for e in soup.select('div[class^=\"def \"]'):  # e is a result from soup.select, so e is also a soup\n",
    "  for ine in e.select('span'):  # since e is a soup, so it can select its sub-parts\n",
    "    if ine.text.strip()=='': continue\n",
    "    print(ine.text.strip())\n",
    "print('-'*80)\n",
    "print(soup.select('b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "eZVAGybN2j0D",
    "outputId": "3acec9a3-0199-41bd-d680-ee8bd091ba5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'hp', 'ei': 'DeoOXtD4BM_IwALbtaDgBA', 'iflsig': 'AAP1E1EAAAAAXg74HYKn6p8iF_gL2mZttpPIxWPuHc9P', 'q': 'fake news detection中文'}\n",
      "約有 37,800,000 項結果 (搜尋時間：0.25 秒) \n",
      "Fake News Challenge\n",
      "Fake news detection - iT 邦幫忙::一起幫忙解決難題，拯救IT 人 ...\n",
      "Detecting Fake News in Social Media Networks - ScienceDirect\n",
      "Fake News Detection on Social Media - sigkdd\n",
      "Fake news detector algorithm works better than a human ...\n",
      "Detecting Fake News on Social Media\n",
      "Web Content Credibility\n",
      "Linking and Mining Heterogeneous and Multi-view Data\n",
      "Proceedings of 6th International Conference in Software ...\n"
     ]
    }
   ],
   "source": [
    "url='https://www.google.com/search'\n",
    "with requests.session() as s:\n",
    "    s.headers['user-agent'] = UserAgent().random\n",
    "    s.headers['accept-language']='zh-TW,zh;q=1.0'\n",
    "    s.headers['accept']='text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'\n",
    "    #s.headers['accept-encoding']='gzip, deflate'\n",
    "    soup = BeautifulSoup(s.get(url).text, 'html5lib')\n",
    "    data = { tag['name']: tag['value'] \n",
    "            for tag in soup.select('input[type=hidden]') if tag.get('value')\n",
    "        }\n",
    "    data.update({'q':'fake news detection中文'})\n",
    "    print(data)\n",
    "    r    = s.get(url, params=data)\n",
    "    #r     = s.get('https://www.google.com/search?q=book&oq=book&aqs=chrome.0.69i59j46j0l6.1111j1j8&sourceid=chrome&ie=UTF-8')\n",
    "    #r.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(r.text, 'html5lib')  \n",
    "    print(soup.select('div[id=\"resultStats\"]')[0].text)\n",
    "    for e in soup.select('h3[class=LC20lb]'):\n",
    "        print(e.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "xAk02pWb4qNi",
    "outputId": "a928202a-d743-42e2-acef-eb0a606bfb94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[問卦] 報告班長3 號角響起\n",
      "Re: [轉錄] 高鈺婷FB：選前一日一闢謠\n",
      "Re: [問卦] 德國之聲在這個時候針對台灣的目的是？\n",
      "[新聞]武漢不明肺炎延燒至社區 當地人怒嗆：外省\n",
      "Re: [問卦] 德國之聲在這個時候針對台灣的目的是？\n",
      "Re: [新聞] 稱主張統一恐成叛國罪 林靜儀道歉辭蔡英\n",
      "Re: [問卦] 德國之聲在這個時候針對台灣的目的是？\n",
      "[問卦] 有那裡能買票的卦嗎?\n",
      "Re: [問卦] 德國之聲在這個時候針對台灣的目的是？\n",
      "Re: [新聞] 稱主張統一恐成叛國罪 林靜儀道歉辭蔡英\n",
      "Re: [問卦] 為什麼寄信一定要dear來dear去？\n",
      "[新聞] 突破！用AI找出乳癌腫瘤 可減少醫師誤判\n",
      "Re: [問卦] 德國之聲在這個時候針對台灣的目的是？\n",
      "Re: [新聞] 稱主張統一恐成叛國罪 林靜儀道歉辭蔡英\n",
      "[新聞] 92歲老翁撞死70歲兄妹 過失致死罪判罰18\n",
      "Re: [新聞] 蔡英文率先停止競選活動 柯文哲:典型行政\n",
      "Re: [問卦] 懶惰的人適合什麼工作？\n",
      "Re: [問卦] 懶惰的人適合什麼工作？\n",
      "[新聞] 情侶找按摩師玩3P 按摩師遭醋男割肌腱\n",
      "[公告] 八卦板板規(2019.08.21)\n",
      "［協尋］12/23南下國道一火燒車事故行車紀錄器\n",
      "[協尋] 行車記錄器 台北市大同區承德路二段\n",
      "[公告] 109年1月1號開始請勿討論選舉民調(宣導)\n",
      "[公告] 赤鴻飛羽，一月份置底閒聊文\n"
     ]
    }
   ],
   "source": [
    "url='https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "url1='https://www.ptt.cc/ask/over18'\n",
    "with requests.session() as s:\n",
    "    s.headers['user-agent'] = UserAgent().random        \n",
    "    s.post(url1, data={'yes':'x'})\n",
    "    r = s.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html5lib')\n",
    "    for e in soup.select('div[class=\"title\"]'):\n",
    "        if e.text.strip()=='': continue\n",
    "        print(e.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QQDutP5jCeuf",
    "outputId": "cce73396-9609-455d-b26a-307a4814dfe6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.ptt.cc'"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "url_stem=url[:url.find('/bbs')]\n",
    "url_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "4PdtH-7I6aIo",
    "outputId": "9821fb53-15a8-4b14-b0fe-1e56e1599ea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to M.1578037171.A.9B4.html.txt ...\n",
      "Saving to M.1578037303.A.C27.html.txt ...\n",
      "Saving to M.1578037321.A.89B.html.txt ...\n",
      "Saving to M.1578037369.A.7B7.html.txt ...\n",
      "Saving to M.1578037425.A.8B1.html.txt ...\n",
      "Saving to M.1578037430.A.FCB.html.txt ...\n",
      "Saving to M.1578037432.A.8FF.html.txt ...\n",
      "Saving to M.1566347622.A.9C7.html.txt ...\n",
      "Saving to M.1577375239.A.9E7.html.txt ...\n",
      "Saving to M.1577676468.A.032.html.txt ...\n",
      "Saving to M.1577753005.A.376.html.txt ...\n",
      "Saving to M.1577812250.A.592.html.txt ...\n"
     ]
    }
   ],
   "source": [
    "url1='https://www.ptt.cc/ask/over18'\n",
    "with requests.session() as s:\n",
    "    s.headers['user-agent'] = UserAgent().random        \n",
    "    s.post(url1, data={'yes':'x'})\n",
    "    r = s.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html5lib')\n",
    "    for e in soup.select('a[href^=\"/bbs/Gossiping/M.\"]'):\n",
    "        if e.text.strip()=='': continue\n",
    "        #print(e.text)\n",
    "        r1 = s.get(url_stem+e['href'])\n",
    "        fn = f\"{e['href'][e['href'].rindex('/')+1:]}.txt\"\n",
    "        print(f\"Saving to {fn} ...\")\n",
    "        with open(fn, 'w') as inf:\n",
    "            sp1 = BeautifulSoup(r1.text, 'html5lib')            \n",
    "            print(sp1.select('div[id=\"main-content\"]')[0].text, file = inf)\n",
    "            #print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "veESODKFXaLS",
    "outputId": "ae23dd18-8bee-4704-d5c2-cb277f7ec22b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 92\n",
      "-rw-r--r-- 1 root root 27704 Jan  3 07:45 M.1566347622.A.9C7.html.txt\n",
      "-rw-r--r-- 1 root root  1954 Jan  3 07:45 M.1577375239.A.9E7.html.txt\n",
      "-rw-r--r-- 1 root root  2086 Jan  3 07:45 M.1577676468.A.032.html.txt\n",
      "-rw-r--r-- 1 root root  4215 Jan  3 07:45 M.1577753005.A.376.html.txt\n",
      "-rw-r--r-- 1 root root  5416 Jan  3 07:45 M.1577812250.A.592.html.txt\n",
      "-rw-r--r-- 1 root root  1002 Jan  3 07:45 M.1578037171.A.9B4.html.txt\n",
      "-rw-r--r-- 1 root root  1320 Jan  3 07:45 M.1578037303.A.C27.html.txt\n",
      "-rw-r--r-- 1 root root  2925 Jan  3 07:45 M.1578037321.A.89B.html.txt\n",
      "-rw-r--r-- 1 root root  1735 Jan  3 07:45 M.1578037369.A.7B7.html.txt\n",
      "-rw-r--r-- 1 root root   498 Jan  3 07:45 M.1578037425.A.8B1.html.txt\n",
      "-rw-r--r-- 1 root root  9574 Jan  3 07:45 M.1578037430.A.FCB.html.txt\n",
      "-rw-r--r-- 1 root root  1429 Jan  3 07:45 M.1578037432.A.8FF.html.txt\n",
      "drwxr-xr-x 1 root root  4096 Dec 18 16:52 sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "26mFE5ZKY8qI",
    "outputId": "765d706e-6805-4515-fad1-c6d5fe4ae76d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作者shengchiu303 (Sheng)看板Gossiping標題[新聞] 弔唁提反年改遭譙爆 王惠美：留言像是時間Fri Jan  3 15:41:58 2020\n",
      "\n",
      "\n",
      "1.媒體來源:\n",
      "自由時報（請參考版規下方的核准媒體名單）\n",
      "\n",
      "\n",
      "2.記者署名\n",
      "\n",
      "記者張聰秋／彰化報導\n",
      "\n",
      "\n",
      "3.完整新聞標題:\n",
      "※ 標題沒有完整寫出來 ---> 依照板規刪除文章\n",
      "弔唁提反年改遭譙爆 王惠美：留言像是楊蕙如般的網軍\n",
      "\n",
      "4.完整新聞內文:\n",
      "\n",
      "黑鷹直升機出事，彰化縣長王惠美臉書發文「追悼英勇將士，請正視軍警消保障」惹議，\n",
      "王惠美今被問及此事反倒認為，謾罵的留言並非彰化縣的親鄉，「有點像是楊蕙如般的網\n",
      "軍進來的...」她強調，自己是希望政府要讓基層的軍警消得到最好的照顧。\n",
      "\n",
      "王惠美今天上午出席縣立成功高中的活動接受媒體訪問時，提出上述的看法，她說，對於\n",
      "一個意外的發生，除了不捨及哀悼之外，也不希望像流星一樣的就過了，期待我們的政府\n",
      "能夠對於軍警消這些特殊的公務人員，第一個工作期間危勞的部分，第二個退休之後相關\n",
      "的權益，希望政府能夠多重視。\n",
      "\n",
      "她說，從立委到現在，她一路以來的原則都沒有變，如何讓這些基層的軍警消得到最好的\n",
      "照顧，這是國家應該要去考慮的！\n",
      "\n",
      "媒體追問，國軍罹難、舉國哀悼，此時提出會不會被覺得是在消費國軍？王惠美回答，「\n",
      "我想公務人員不會覺得我在消費吧！」媒體再問，會不會覺得遭到留言攻擊？王惠美說，\n",
      "那些留言幾乎都不是彰化縣鄉親，有點像是楊蕙如般的網軍進來的。\n",
      "\n",
      "她說，她主要的目的還是希望政府能夠對這些危勞的軍警消，他們在職的危險加給，以及\n",
      "他們退休之後的照顧，應該要給他們一個好的照顧，讓他們在工作的範圍內無後顧之憂。\n",
      "\n",
      "https://i.imgur.com/VBdc4jJ.jpg\n",
      "\n",
      "5.完整新聞連結 (或短網址):\n",
      "※ 當新聞連結過長時，需提供短網址方便網友點擊\n",
      "https://news.ltn.com.tw/news/politics/breakingnews/3028564\n",
      "\n",
      "6.備註:\n",
      "※ 一個人一天只能張貼一則新聞，被刪或自刪也算額度內，超貼者水桶，請注意\n",
      "\n",
      "發生這樣的事還在帶風向反年改\n",
      "\n",
      "沒意外她還是可以連任啦。\n",
      "\n",
      "https://i.imgur.com/Zt10NkB.jpg\n",
      "\n",
      "--\n",
      "※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 101.12.29.90 (臺灣)\n",
      "※ 文章網址: https://www.ptt.cc/bbs/Gossiping/M.1578037321.A.89B.html\n",
      "※ 編輯: shengchiu303 (101.12.29.90 臺灣), 01/03/2020 15:43:22\n",
      "噓 s820912gmail: 嗯嗯嗯  49.214.245.12 01/03 15:42\n",
      "推 wwf0322: 什麼都扯卡神是唯一正解 49.215.186.136 01/03 15:43\n",
      "噓 e0936276915: 這個腦袋就跟韓導說罷免遊行不是高雄  180.204.89.85 01/03 15:44\n",
      "→ e0936276915: 人不能參加一樣沒邏輯  180.204.89.85 01/03 15:44\n",
      "噓 yor: 被罵就東拉西扯 101.13.160.108 01/03 15:44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat M.1578037321.A.89B.html.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "FvDXxh6-rF0p",
    "outputId": "446475cf-2f20-4eb8-d4b6-c03a60338fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"listen\">\n",
      "<b>This is a test.</b>\n",
      "</span>, <span class=\"listen2\">\n",
      "This is a test 2.\n",
      "</span>, <span class=\"listen3\">\n",
      "this is a test 3.\n",
      "</span>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.select('span[class^=\"listen\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "QyMrTTb0xFAR",
    "outputId": "45f25926-28f7-4ac8-d068-50793b126662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"listen\">\n",
      "<b>This is a test.</b>\n",
      "</span>, <span class=\"listen2\">\n",
      "This is a test 2.\n",
      "</span>, <span class=\"listen3\">\n",
      "this is a test 3.\n",
      "</span>]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(soup.find_all('span', {'class': re.compile('^listen')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "colab_type": "code",
    "id": "VQyicf4_yXpj",
    "outputId": "0299d1d1-52f4-40f2-c1c0-fe7caec1bc68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"listen\">\n",
      "<b>This is a test.</b>\n",
      "</span>, <span class=\"listen2\">\n",
      "This is a test 2.\n",
      "</span>, <span class=\"listen3\">\n",
      "this is a test 3.\n",
      "</span>]\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all('span', class_=re.compile('^listen')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "dKfNcqLnrgx2",
    "outputId": "7265f5e0-bc83-4099-dd29-8fe7c1b3637e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"listen\">\n",
      "<b>This is a test.</b>\n",
      "</span>\n"
     ]
    }
   ],
   "source": [
    "print(soup.select_one('span[class^=\"listen\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "Q6Xvlq49rkW5",
    "outputId": "266306a9-46e6-49a2-c2b9-cf05d1f243a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"listen2\">\n",
      "This is a test 2.\n",
      "</span>\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('span', class_=re.compile(\"^listen.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBZ2r3qgl4sF"
   },
   "source": [
    "#### Applications:\n",
    "- ['https://tw.dictionary.search.yahoo.com/'](https://tw.dictionary.search.yahoo.com/)\n",
    "- ['https://dictionary.cambridge.org/dictionary/english-chinese-traditional/'](https://dictionary.cambridge.org/dictionary/english-chinese-traditional/)\n",
    "- ['http://www.liveabc.com/site/Online_Store/resource/essential_english/essential.asp'](http://www.liveabc.com/site/Online_Store/resource/essential_english/essential.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1We_Z0oxl2Ly"
   },
   "outputs": [],
   "source": [
    "url = 'http://www.liveabc.com/site/Online_Store/resource/essential_english/essential.asp'\n",
    "rs = requests.Session()\n",
    "rs.headers['user-agent'] = UserAgent().random  #'Mozilla/5.0'\n",
    "#rs.post(url, data=payload)\n",
    "res = rs.get(url)\n",
    "soup = BeautifulSoup(res.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ODYZHF7miBE"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "c = 0\n",
    "for link in soup.select('a'):\n",
    "    if 'seq' in link['href']: # seq = 1 ~ 99\n",
    "       # get seq\n",
    "       id = link['href'][link['href'].rindex('seq=')+4:]\n",
    "       url2 = url_stem + link['href']\n",
    "       res = rs.get(url2)\n",
    "       sub_soup = BeautifulSoup(res.content, 'html5lib')\n",
    "       with open(f'Lesson{id}.txt', 'w') as f:\n",
    "            for td in sub_soup.find_all('td', attrs={'bgcolor':'#FFFFCC'}):                \n",
    "                print(td.text, file=f)\n",
    "       c+=1\n",
    "       if c>=5: break\n",
    "       rs.close()\n",
    "       sleep(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LcHZGYdYqjCf"
   },
   "outputs": [],
   "source": [
    "def lookup_cambridge(word, _max=2):\n",
    "    url=\"https://dictionary.cambridge.org/dictionary/english-chinese-traditional/\"\n",
    "    with requests.session() as s:\n",
    "        s.headers['user-agent'] = UserAgent().random        \n",
    "        r    = s.get(url+word)\n",
    "        soup = BeautifulSoup(r.content, 'html5lib')\n",
    "        \n",
    "        en=''\n",
    "        cnt = 1\n",
    "        for x in soup.select('div[class^=\"def ddef_d\"]'):\n",
    "            if en: \n",
    "                en += '; '\n",
    "                cnt += 1\n",
    "            en += x.text\n",
    "            if cnt >= _max: break\n",
    "            \n",
    "        ch=''\n",
    "        cnt = 1\n",
    "        for x in soup.select('div[class^=\"def-body ddef_b\"]'):\n",
    "            x = x.find('span', class_='trans dtrans dtrans-se')\n",
    "            if ch: \n",
    "                ch += '; '\n",
    "                cnt += 1\n",
    "            ch += x.text\n",
    "            if cnt >= _max: break\n",
    "        return ch, en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJVB8AtQ1uLN"
   },
   "source": [
    "### Selenium\n",
    "- ref: [Using Selenium with Google Colaboratory](https://darektidwell.com/using-selenium-with-google-colaboratory/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ixsu2vEU2Fne"
   },
   "source": [
    "**install chromium, its driver, and selenium:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_mCsfN6_Kn0H",
    "outputId": "a2024bef-ec8d-4bee-fd0b-c2ac622aafb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
      "\u001b[K     |████████████████████████████████| 911kB 8.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n",
      "Hit:1 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:5 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
      "Get:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Ign:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Get:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
      "Get:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
      "Get:13 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,749 kB]\n",
      "Get:14 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [844 kB]\n",
      "Get:15 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [81.6 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [761 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,057 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,322 kB]\n",
      "Get:20 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [30.4 kB]\n",
      "Fetched 6,118 kB in 2s (2,754 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following package was automatically installed and is no longer required:\n",
      "  libnvidia-common-430\n",
      "Use 'apt autoremove' to remove it.\n",
      "The following additional packages will be installed:\n",
      "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
      "Suggested packages:\n",
      "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
      "The following NEW packages will be installed:\n",
      "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
      "  chromium-codecs-ffmpeg-extra\n",
      "0 upgraded, 4 newly installed, 0 to remove and 19 not upgraded.\n",
      "Need to get 72.7 MB of archives.\n",
      "After this operation, 259 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 79.0.3945.79-0ubuntu0.18.04.1 [1,079 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 79.0.3945.79-0ubuntu0.18.04.1 [64.5 MB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 79.0.3945.79-0ubuntu0.18.04.1 [3,094 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 79.0.3945.79-0ubuntu0.18.04.1 [3,976 kB]\n",
      "Fetched 72.7 MB in 1s (67.9 MB/s)\n",
      "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
      "(Reading database ... 135004 files and directories currently installed.)\n",
      "Preparing to unpack .../chromium-codecs-ffmpeg-extra_79.0.3945.79-0ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking chromium-codecs-ffmpeg-extra (79.0.3945.79-0ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package chromium-browser.\n",
      "Preparing to unpack .../chromium-browser_79.0.3945.79-0ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking chromium-browser (79.0.3945.79-0ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package chromium-browser-l10n.\n",
      "Preparing to unpack .../chromium-browser-l10n_79.0.3945.79-0ubuntu0.18.04.1_all.deb ...\n",
      "Unpacking chromium-browser-l10n (79.0.3945.79-0ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package chromium-chromedriver.\n",
      "Preparing to unpack .../chromium-chromedriver_79.0.3945.79-0ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking chromium-chromedriver (79.0.3945.79-0ubuntu0.18.04.1) ...\n",
      "Setting up chromium-codecs-ffmpeg-extra (79.0.3945.79-0ubuntu0.18.04.1) ...\n",
      "Setting up chromium-browser (79.0.3945.79-0ubuntu0.18.04.1) ...\n",
      "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
      "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
      "Setting up chromium-chromedriver (79.0.3945.79-0ubuntu0.18.04.1) ...\n",
      "Setting up chromium-browser-l10n (79.0.3945.79-0ubuntu0.18.04.1) ...\n",
      "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Processing triggers for mime-support (3.60ubuntu1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!apt-get update # to update ubuntu to correctly run apt install\n",
    "!apt install chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3BHb-0h2WLX"
   },
   "source": [
    "**set options to be headless**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8_PNNZwK_3L"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "from selenium import webdriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument(\"--lang=zh-TW\")\n",
    "wd = webdriver.Chrome('chromedriver', options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "JeSkjSNq2lxc",
    "outputId": "84850526-3865-4f04-9098-300dc437e277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake News Challenge\n",
      "Fake news detection - iT 邦幫忙::一起幫忙解決難題，拯救IT 人 ...\n",
      "Fake News Detection on Social Media - sigkdd\n",
      "Detecting Fake News in Social Media Networks - ScienceDirect\n",
      "Fake news detector algorithm works better than a human ...\n",
      "Detecting Fake News on Social Media\n",
      "IFLA -- How To Spot Fake News\n",
      "How to combat fake news and disinformation\n",
      "Web Content Credibility\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "url='https://www.google.com.tw/'\n",
    "q='fake news detection中文'\n",
    "\n",
    "# open it, go to a website, and get results\n",
    "wd.get(url)\n",
    "wd.implicitly_wait(10)\n",
    "textbox = wd.find_element_by_xpath(\"//input[@name='q'][@type='text']\")\n",
    "textbox.send_keys(q)\n",
    "textbox.send_keys(Keys.ENTER)\n",
    "#wd.find_element_by_xpath(\"//input[@name='btnK'][@type='submit']\").click()\n",
    "soup=BeautifulSoup(wd.page_source, 'html5lib')\n",
    "for e in soup.select('h3[class^=\"LC20\"]'):\n",
    "        print(e.text)\n",
    "wd.quit()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DemoBeautifulSoup.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
